{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089d9473",
   "metadata": {},
   "source": [
    "## Pipelines para pré-processamento de dados\n",
    "Pipelines  são  uma  maneira  eficiente  de  encadear  várias  etapas  de  pré-processamento  de  dados  e  modelagem.  Isso  é  crucial  para  manter  o  código organizado e facilitar a reprodução de experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe030d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Carregar um dataset exemplo\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir o pipeline\n",
    "pipeline = Pipeline([\n",
    "   ('scaler', StandardScaler()),\n",
    "   ('pca', PCA(n_components=3)),\n",
    "   ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Treinar o pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Mostrar previsões\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada7f0f",
   "metadata": {},
   "source": [
    "## Otimização de hiperparâmetros com grid search\n",
    "O Grid Search é uma técnica poderosa para encontrar a combinação ideal de hiperparâmetros para um modelo, o que resulta em um desempenho otimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1938d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__max_depth': 10, 'classifier__n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Carregar um dataset exemplo\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir o pipeline\n",
    "pipeline = Pipeline([\n",
    "   ('scaler', StandardScaler()),\n",
    "   ('pca', PCA(n_components=3)),\n",
    "   ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Definir a grade de parâmetros para GridSearch\n",
    "param_grid = {\n",
    "   'classifier__n_estimators': [50, 100, 150],\n",
    "   'classifier__max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Executar GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar os melhores parâmetros encontrados\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfc843",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "* Document Loaders and Utils: Essa parte do LangChain se refere aos carregadores de documentos e utilitários. Eles são responsáveis por ler, processar e carregar documentos de diversas fontes (arquivos de texto, PDFs, bancos de dados, etc.) para que possam ser utilizados nas operações de processamento de linguagem natural.\n",
    "\n",
    "* Prompts: Prompts são instruções ou perguntas fornecidas ao modelo de linguagem para gerar respostas ou realizar uma tarefa específica. Eles são fundamentais para direcionar o comportamento do modelo de linguagem.\n",
    "\n",
    "* Chains (ou cadeias): Elas representam uma sequência de etapas ou operações que são realizadas em conjunto para alcançar um objetivo específico. Embora o uso de um único LLM possa ser suficiente para tarefas mais simples, LangChain fornece uma interface padrão e algumas implementações comumente usadas para encadear LLMs para aplicações mais complexas, entre si ou com outros módulos especializados.\n",
    "\n",
    "* LLM (Large Language Models): LLMs são modelos de linguagem de grande escala, como GPT-4, BERT, e outros, que são utilizados para realizar várias tarefas de processamento de linguagem natural, como tradução, sumarização, e geração de texto.\n",
    "\n",
    "* Agents: Eles são componentes que agem de forma autônoma para realizar tarefas específicas baseadas em entradas e em modelos de linguagem. Eles podem tomar decisões e realizar ações com base em regras ou aprendizagem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce09107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_917839/2336885399.py:17: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n",
      "/tmp/ipykernel_917839/2336885399.py:17: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  llm = Ollama(\n",
      "/tmp/ipykernel_917839/2336885399.py:29: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain_sentimento = LLMChain(llm=llm, prompt=template_sentimento)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m chain_sentimento \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mtemplate_sentimento)\n\u001b[1;32m     30\u001b[0m chain_resumo \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mtemplate_resumo)\n\u001b[0;32m---> 32\u001b[0m caminho_arquivo \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoticias.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(caminho_arquivo):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO arquivo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaminho_arquivo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m não foi encontrado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "def carregar_documentos(caminho_arquivo):\n",
    "    loader = TextLoader(caminho_arquivo)\n",
    "    documentos = loader.load()\n",
    "    return documentos\n",
    "\n",
    "def limpar_texto(texto):\n",
    "    return texto.strip()\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2\",\n",
    "    num_gpu=0,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "prompt_sentimento = \"Analise o sentimento do seguinte texto em português: {text}\"\n",
    "prompt_resumo = \"Gere um resumo em português para o seguinte texto: {text}\"\n",
    "\n",
    "template_sentimento = PromptTemplate(input_variables=[\"text\"], template=prompt_sentimento)\n",
    "template_resumo = PromptTemplate(input_variables=[\"text\"], template=prompt_resumo)\n",
    "\n",
    "chain_sentimento = LLMChain(llm=llm, prompt=template_sentimento)\n",
    "chain_resumo = LLMChain(llm=llm, prompt=template_resumo)\n",
    "\n",
    "caminho_arquivo = os.path.join(os.path.dirname(__file__), \"noticias.txt\")\n",
    "\n",
    "if not os.path.exists(caminho_arquivo):\n",
    "    raise FileNotFoundError(f\"O arquivo {caminho_arquivo} não foi encontrado.\")\n",
    "\n",
    "documentos = carregar_documentos(caminho_arquivo)\n",
    "\n",
    "for doc in documentos:\n",
    "    texto_limpo = limpar_texto(doc.page_content)\n",
    "\n",
    "    resultado_sentimento = chain_sentimento.run({\"text\": texto_limpo})\n",
    "    resultado_resumo = chain_resumo.run({\"text\": texto_limpo})\n",
    "\n",
    "    print(f\"Notícia: {texto_limpo}\")\n",
    "    print(f\"Sentimento: {resultado_sentimento}\")\n",
    "    print(f\"Resumo: {resultado_resumo}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
